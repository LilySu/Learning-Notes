{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word_Phrase_FastText.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVjZCnCB1llQ",
        "colab_type": "code",
        "outputId": "83d3b078-4b94-420c-8b2b-3f4d5933ef40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1xsjMh-mVzK",
        "colab_type": "code",
        "outputId": "5a46fbeb-d7f6-410c-9d84-5f4255c28377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        }
      },
      "source": [
        "!git clone https://github.com/facebookresearch/fastText.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'fastText'...\n",
            "remote: Enumerating objects: 122, done.\u001b[K\n",
            "remote: Counting objects: 100% (122/122), done.\u001b[K\n",
            "remote: Compressing objects: 100% (77/77), done.\u001b[K\n",
            "remote: Total 3653 (delta 53), reused 67 (delta 27), pack-reused 3531\u001b[K\n",
            "Receiving objects: 100% (3653/3653), 8.16 MiB | 9.60 MiB/s, done.\n",
            "Resolving deltas: 100% (2278/2278), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3HcyNt15Sxn",
        "colab_type": "code",
        "outputId": "31c5f418-812c-461e-db67-88c523fcb05c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "! sudo pip install /content/fastText/."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./fastText\n",
            "Requirement already satisfied: pybind11>=2.2 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (2.4.3)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (42.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fasttext==0.9.1) (1.17.4)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.1-cp36-cp36m-linux_x86_64.whl size=2854934 sha256=e4c0dcafde5e2b4d82dac7a9ce76a20fec4fee0996c7aca388647f91392efc10\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-k5gxg3oj/wheels/a1/9f/52/696ce6c5c46325e840c76614ee5051458c0df10306987e7443\n",
            "Successfully built fasttext\n",
            "Installing collected packages: fasttext\n",
            "Successfully installed fasttext-0.9.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaRZpRpHo0uH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import fasttext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AKzD21T4acWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#breaks reviews up into individual words, tallies up word occurrences and extracts phrases where word appears.\n",
        "#spacy and scattertext are not used because the results are decent without it and for companies with few reviews, compute time is instant.\n",
        "\n",
        "import math\n",
        "import json\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from lxml import html\n",
        "from requests import Session\n",
        "from concurrent.futures import ThreadPoolExecutor as Executor\n",
        "import requests\n",
        "import re\n",
        "pd.options.display.max_rows = 999\n",
        "pd.options.display.max_columns = 999\n",
        "pd.set_option('display.max_colwidth', 1000)\n",
        "base_url = \"https://www.yelp.com/biz/\" \n",
        "api_url = \"/review_feed?sort_by=date_desc&start=\"\n",
        "bid = 'OCTiJwvjoK81WoDwsTkFvA'\n",
        "# bid = 'CBf1A4zCRNH-jsrJh48RsA'\n",
        "\n",
        "\n",
        "class Scraper():\n",
        "    def __init__(self):\n",
        "        self.data = pd.DataFrame()\n",
        "\n",
        "    def get_data(self, n, bid=bid):\n",
        "        with Session() as s:\n",
        "            with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n",
        "                r = json.loads(resp.content) #converts json response into a dictionary\n",
        "                _html = html.fromstring(r['review_list']) #loads from dictionary\n",
        "\n",
        "                dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n",
        "                reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n",
        "                ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n",
        "\n",
        "                df = pd.DataFrame([dates, reviews, ratings]).T\n",
        "\n",
        "                self.data = pd.concat([self.data,df])\n",
        "\n",
        "    def scrape(self): #makes it faster\n",
        "        # multithreaded looping\n",
        "        with Executor(max_workers=40) as e:\n",
        "            list(e.map(self.get_data, range(10)))\n",
        "\n",
        "s = Scraper()\n",
        "s.scrape()\n",
        "df = s.data\n",
        "df = df.dropna()\n",
        "\n",
        "df['word_segments_unpacked'] = df[1].apply(lambda x: x[1:-1].split(' '))#turn string comma separated list per word\n",
        "\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].astype(str)\n",
        "df['word_segments_unpacked'] = df['word_segments_unpacked'].apply(lambda x: ''.join([str(i) for i in x]))\n",
        "phrase_count = df[['word_segments_unpacked', 2]]\n",
        "\n",
        "\n",
        "s= phrase_count.apply(lambda x: pd.Series(x['word_segments_unpacked']),axis=1).stack().reset_index(level=1, drop=True)\n",
        "s.name = 'word_segments_unpacked'\n",
        "\n",
        "phrase_count = phrase_count.drop('word_segments_unpacked', axis=1).join(s)\n",
        "phrase_count = pd.DataFrame(df['word_segments_unpacked'].str.split(',').tolist(), index=df[2]).stack()\n",
        "\n",
        "phrase_count = phrase_count.reset_index()[[0, 2]] # var1 variable is currently labeled 0\n",
        "phrase_count.columns = ['word_segments_unpacked', 'ratings'] # renaming var1\n",
        "phrase_count = phrase_count.reset_index(drop=False)\n",
        "replace_dict_phrase_count = {'[':'',']':'','-':'','!':'','.':'',' ':'',\"'\":''}\n",
        "for key in replace_dict_phrase_count.keys():\n",
        "  phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(key, replace_dict_phrase_count[key])\n",
        "phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.lower()\n",
        "\n",
        "stopwords = [')','(','\\(','\\xa0','0','1','2','3','4','5','6','7','8','9','/','$',\"'d\",\"'ll\",\"'m\",'+','maybe','from','first','here','only','put','where','got','sure','definitely','food','yet','our','go','since','really','very','two',\"n't\",'with','if',\"'s\",'which','came','all','me','(',')','makes','make','were','immediately','get','been','ahead','also','that','one','have','see','what','to','we','had','.',\"'re\",'it','or','he','she','we','us','how','went','no','\"','of','has','by','bit','thing','place','so','ok','and','they','none','was','you',\"'ve\",'did','be','and','but','is','as','&','you','has','-',':','and','had','was','him','so','my','did','would','her','him','it','is','by','bit','thing','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', 'check', 'i\"m', \"it's\",'they', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n",
        "def filter_stopwords(text):\n",
        "  for i in str(text):\n",
        "    if i not in stopwords:\n",
        "      return str(text)\n",
        "\n",
        "#if item in stopwords list partially matches, delete, single letters like 'i' would be deleted from inside individual words if in list\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(stopwords)]\n",
        "#if the following words fully matches, filter out\n",
        "full_match_list = ['i','a','an','am','at','are','in','on','for','','\\xa0\\xa0','\\xa0','\\(']\n",
        "phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(full_match_list)]\n",
        "\n",
        "#pivot table ratings\n",
        "phrase_count_pivot = pd.pivot_table(phrase_count, index='word_segments_unpacked', columns='ratings', aggfunc='count', fill_value=0)\n",
        "phrase_count_pivot.columns = [''.join(col).strip() for col in phrase_count_pivot.columns.values]#flatten index levels part 1\n",
        "phrase_count_pivot = pd.DataFrame(phrase_count_pivot.to_records())#flatten index levels part 2\n",
        "\n",
        "#if there are no _# star reviews, add a column of zeros\n",
        "required_column_names = ['index1.0 star rating', 'index2.0 star rating','index3.0 star rating','index4.0 star rating','index5.0 star rating']\n",
        "for i in required_column_names:\n",
        "  if i not in phrase_count_pivot.columns:\n",
        "    phrase_count_pivot[i] = 0\n",
        "phrase_count_pivot.sample(10)\n",
        "\n",
        "#replace the original count by getting an exaggerated scaled tally of reviews to calculate score\n",
        "phrase_count_pivot['index1.0 star rating'] = phrase_count_pivot['index1.0 star rating']*(-2)\n",
        "phrase_count_pivot['index2.0 star rating'] = phrase_count_pivot['index2.0 star rating']*(-1)\n",
        "phrase_count_pivot['index3.0 star rating'] = phrase_count_pivot['index3.0 star rating']*(-0.1)\n",
        "phrase_count_pivot['index4.0 star rating'] = phrase_count_pivot['index4.0 star rating']*(1)\n",
        "phrase_count_pivot['index5.0 star rating'] = phrase_count_pivot['index5.0 star rating']*(2)\n",
        "\n",
        "#get a total score from the sum of exaggerated scores\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['index1.0 star rating'] + phrase_count_pivot['index2.0 star rating'] + phrase_count_pivot['index3.0 star rating'] + phrase_count_pivot['index4.0 star rating'] + phrase_count_pivot['index5.0 star rating']\n",
        "\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].div(phrase_count_pivot['score'].max(), axis=0)#normalize\n",
        "phrase_count_pivot['score'] = phrase_count_pivot['score'].round(decimals=4)#round to 4 decimal places\n",
        "phrase_count_pivot = phrase_count_pivot.sort_values(by=('score'), ascending=False)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rcIRFVc_THXE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('\\(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('\\\\', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace('(', '')\n",
        "phrase_count_pivot['word_segments_unpacked'] = phrase_count_pivot['word_segments_unpacked'].str.replace(')', '')#without these, errors incurr\n",
        "\n",
        "\n",
        "worst_terms_list = [] \n",
        "top_terms_list = []\n",
        "x,y = phrase_count_pivot.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in range(math.ceil(x/2)):\n",
        "# for i in range(0,30):\n",
        "  try:\n",
        "    new_df = df[df[1].str.contains(phrase_count_pivot['word_segments_unpacked'].iloc[i])]#if word appears in review, create a dataframe with each row being the word occurring in a different review\n",
        "    neg_first_df = new_df.sort_values(by=2, ascending=True)#rank the dataframe with worst reviews first\n",
        "    pos_first_df = new_df.sort_values(by=2, ascending=False)#rank the dataframe with most positive reviews first\n",
        "    if neg_first_df[1].iloc[0] not in worst_terms_list:#get the lowest star rating review\n",
        "      worst_terms_list.append(neg_first_df[1].iloc[0])#prevent duplicates\n",
        "    if pos_first_df[1].iloc[0] not in top_terms_list:#get the highest star rating review\n",
        "      top_terms_list.append(pos_first_df[1].iloc[0])\n",
        "  except IndexError as e:\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9zTFl3xTEzc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "negative_list = []\n",
        "for i in range(-30,0):#take the worst 30 terms\n",
        "  for list_of_words in worst_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word: #find word occurrence in original comma separated word list of reviews\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:#if there are less than 30 words after stopword filtering, just get the first word and its occurrence in the original review\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict_string_from_phrases.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "negative_df = pd.DataFrame(negative_list)\n",
        "negative_df = negative_df.reset_index(drop=False)\n",
        "negative_df = negative_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = negative_df.drop_duplicates(subset='term')\n",
        "x,y = negative_df.shape#tuple unpacking to get the length of the dataframe\n",
        "if x < 10:\n",
        "  for i in range(-40,-30):\n",
        "    for list_of_words in worst_terms_list:\n",
        "      word_list = list_of_words.split(' ')\n",
        "      for word in word_list:\n",
        "        word = word.replace(',','')\n",
        "        word = word.replace('.','')\n",
        "        try:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "        except IndexError as e:\n",
        "          if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "            try:\n",
        "              index = word_list.index(word)\n",
        "              string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "              replace_dict_string_from_phrases= {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "              for key in replace_dict_string_from_phrases.keys():\n",
        "                string_from_phrases=string_from_phrases.replace(key, replace_dict_string_from_phrases[key])\n",
        "              negative_list.append(string_from_phrases)\n",
        "            except ValueError as e:\n",
        "              pass\n",
        "negative_df_addon = pd.DataFrame(negative_list)\n",
        "negative_df_addon = negative_df_addon.reset_index(drop=False)\n",
        "negative_df_addon = negative_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "negative_df = pd.concat([negative_df, negative_df_addon])\n",
        "negative_df = negative_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LQNIOv2H67ZN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "positive_list = []\n",
        "for i in range(0,30):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try: \n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "  positive_list.append(string_from_phrases)\n",
        "positive_df = pd.DataFrame(positive_list)\n",
        "positive_df = positive_df.reset_index(drop=False)\n",
        "positive_df = positive_df.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = positive_df.drop_duplicates(subset='term')\n",
        "x,y = positive_df.shape#tuple unpacking to get the length of the dataframe\n",
        "for i in range(30,40):\n",
        "  for list_of_words in top_terms_list:\n",
        "    word_list = list_of_words.split(' ')\n",
        "    for word in word_list:\n",
        "      word = word.replace(',','')\n",
        "      word = word.replace('.','')\n",
        "      try:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[i] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "      except IndexError as e:\n",
        "        if phrase_count_pivot['word_segments_unpacked'].iloc[0] == word:\n",
        "          try:\n",
        "            index = word_list.index(word)\n",
        "            string_from_phrases = ','.join(word_list[max(0,index-5):min(index+20, len(word_list))])\n",
        "            replace_dict = {'\\xa0':'',' ':'',',':' ',' .':'.','!':'','[':'',']':'','\\n':'','1':'','2':'','3':'','4':'','5':'','6':'','7':'','8':'','9':'','0':'','/':'',\"'\":\"'\",\"'\":''}\n",
        "            for key in replace_dict.keys():\n",
        "              string_from_phrases=string_from_phrases.replace(key, replace_dict[key])\n",
        "            negative_list.append(string_from_phrases)\n",
        "          except ValueError as e:\n",
        "            pass\n",
        "positive_df_addon = pd.DataFrame(negative_list)\n",
        "positive_df_addon = positive_df_addon.reset_index(drop=False)\n",
        "positive_df_addon = positive_df_addon.rename(columns={'index':'score', 0 : 'term'})\n",
        "positive_df = pd.concat([positive_df, positive_df_addon])\n",
        "positive_df = positive_df.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PN4eR2M5YOa6",
        "colab_type": "code",
        "outputId": "c70e414a-6566-410b-8c17-8432c7063cc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "phrase_count_pivot.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>word_segments_unpacked</th>\n",
              "      <th>index1.0 star rating</th>\n",
              "      <th>index2.0 star rating</th>\n",
              "      <th>index3.0 star rating</th>\n",
              "      <th>index4.0 star rating</th>\n",
              "      <th>index5.0 star rating</th>\n",
              "      <th>score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>778</th>\n",
              "      <td>great</td>\n",
              "      <td>-4</td>\n",
              "      <td>-3</td>\n",
              "      <td>-0.6</td>\n",
              "      <td>15</td>\n",
              "      <td>80</td>\n",
              "      <td>1.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>888</th>\n",
              "      <td>italian</td>\n",
              "      <td>-8</td>\n",
              "      <td>-1</td>\n",
              "      <td>-0.9</td>\n",
              "      <td>12</td>\n",
              "      <td>80</td>\n",
              "      <td>0.9394</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    word_segments_unpacked  index1.0 star rating  index2.0 star rating  \\\n",
              "778                  great                    -4                    -3   \n",
              "888                italian                    -8                    -1   \n",
              "\n",
              "     index3.0 star rating  index4.0 star rating  index5.0 star rating   score  \n",
              "778                  -0.6                    15                    80  1.0000  \n",
              "888                  -0.9                    12                    80  0.9394  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "morWsl9YyP12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df.to_csv(r'pandas.txt', header=None, index=None, sep=' ', mode='a')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z-kr7F5bpGJe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = fasttext.train_unsupervised('pandas.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAydbEWxSrv0",
        "colab_type": "code",
        "outputId": "b71a7873-bcd1-4b0c-a3b5-730f85a377f1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        }
      },
      "source": [
        "negative_df.head(2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>score</th>\n",
              "      <th>term</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>restaurant is a landmark - home to the most horrible restaurant host in the known universe He seated my companion and I at a small</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>has got to be the worst Italian restaurant i have ever ate in in my life. three of us decided to just order appetizers but</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   score  \\\n",
              "0      0   \n",
              "1      1   \n",
              "\n",
              "                                                                                                                                 term  \n",
              "0  restaurant is a landmark - home to the most horrible restaurant host in the known universe He seated my companion and I at a small  \n",
              "1          has got to be the worst Italian restaurant i have ever ate in in my life. three of us decided to just order appetizers but  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9cQ3mcOTxfF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "snippet = positive_df.head(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FW2yQJw7RwaH",
        "colab_type": "code",
        "outputId": "0e70dff8-2339-40dc-cb3d-eab72f64e1a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "similar_list = []\n",
        "\n",
        "for i in positive_df['term']:\n",
        "  model.get_nearest_neighbors(i)\n",
        "  for i in (model.get_nearest_neighbors(i)):\n",
        "    x,y = i\n",
        "    similar_list.append(y)\n",
        "print(similar_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['fantastic!', 'go', 'area.', \"It's\", 'expect', 'favorite', 'fast', 'far', 'reasonably', 'Prices', 'expect', 'reasonably', 'inviting.', 'area.', 'expectations', 'beginning', 'outstanding.', 'reasonable', 'fantastic!', 'Prices', 'Tiramisu', 'Gnocchi', 'portions.', 'portions', 'gnocchi', 'Very', '\"Good', 'it!', 'authentic.', 'Absolutely', 'expect', 'reasonable', 'reasonably', 'eat', 'starts', 'cute', 'inviting', 'seating.', 'welcoming', 'beginning', 'authentic.', '\"Good', 'portions', 'portions.', 'atmosphere.', 'Atmosphere', 'Very', 'portions,', 'food!', 'Beautiful', '\\xa0Service', 'priced', 'service,', '\"Service', 'atmosphere.', 'nice', 'service.', 'great,', 'great.', 'Service', 'gives', 'unbeatable', \"Don't\", 'heart', 'visit', 'point', 'lots', 'annual', 'guy', 'Overall,', 'miss', 'gem', 'Definitely', 'feast', 'return.\"', 'fantastic.\"', 'return', 'back.\"', 'again!\"', 'customer', 'Tiramisu', 'Gnocchi', 'worth', 'gnocchi', 'it!', 'guy', 'point', 'Overall,', 'Overall', 'fast', 'Alfredo', 'sauce,', 'Vodka', 'Shrimp', 'pasta.', 'meatballs.', 'cheesecake', 'antipasto', 'meatballs', 'sauce']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGvh873uUI4M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stopwords = ['fantastic','go','beginning','reasonable','outstanding','good','expect','eat','beginning',]\n",
        "def filter_stopwords(text):\n",
        "  for i in str(text):\n",
        "    if i not in stopwords:\n",
        "      return str(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0y0-XesSZ7G",
        "colab_type": "code",
        "outputId": "ff20dfea-0939-48a8-d710-ab0504f11a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(similar_list)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "17840"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qu_5aZDg2U8H",
        "colab_type": "code",
        "outputId": "3e259634-4b7c-483f-8bea-b4c61071ff97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "for i in phrase_count_pivot['word_segments_unpacked']:\n",
        "  model.get_nearest_neighbors(i)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9999875426292419, 'great.'),\n",
              " (0.9999858140945435, 'great'),\n",
              " (0.9999794363975525, 'since'),\n",
              " (0.9999735355377197, 'take'),\n",
              " (0.9999716877937317, 'our'),\n",
              " (0.9999716877937317, 'host'),\n",
              " (0.9999700784683228, 'like'),\n",
              " (0.9999678134918213, 'never'),\n",
              " (0.9999666213989258, 'it.'),\n",
              " (0.9999657273292542, 'long')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTJ6esCq7Tf5",
        "colab_type": "text"
      },
      "source": [
        "###Word Analogies"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8AS0RTB7Sb3",
        "colab_type": "text"
      },
      "source": [
        "In a similar spirit, one can play around with word analogies. For example, we can see if our model can guess what is to France, what Berlin is to Germany.\n",
        "\n",
        "This can be done with the analogies functionality. It takes a word triplet (like Germany Berlin France) and outputs the analogy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gc1E85lH6hix",
        "colab_type": "code",
        "outputId": "affaa1b4-7e6d-4899-f8a4-419c993b6fac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "model.get_analogies('food', 'burnt','drink')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9998505711555481, 'Malay'),\n",
              " (0.9998475313186646, 'three'),\n",
              " (0.9998438358306885, 'about'),\n",
              " (0.9998437166213989, 'Malaysian'),\n",
              " (0.9998391270637512, 'think'),\n",
              " (0.9998390078544617, 'owners'),\n",
              " (0.9998306632041931, 'tasted'),\n",
              " (0.999829113483429, '\"We'),\n",
              " (0.9998281598091125, 'will'),\n",
              " (0.9998226165771484, 'more')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pH-RfnUSxLM7",
        "colab_type": "code",
        "outputId": "e5619f7a-50c1-46c0-82cb-5e73efff83e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        }
      },
      "source": [
        "model.get_nearest_neighbors('gearshift')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0.9999086260795593, 'much'),\n",
              " (0.9999054670333862, 'favorite'),\n",
              " (0.99989914894104, 'around'),\n",
              " (0.9998939037322998, 'restaurants'),\n",
              " (0.9998882412910461, 'usually'),\n",
              " (0.999884307384491, 'delicious.'),\n",
              " (0.9998836517333984, 'just'),\n",
              " (0.9998722672462463, 'most'),\n",
              " (0.9998640418052673, 'here.'),\n",
              " (0.9998615980148315, \"It's\")]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6736mqSJzoYz",
        "colab_type": "code",
        "outputId": "f7db7bc1-cc11-4fa6-eddc-f54543588952",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        }
      },
      "source": [
        "model.get_word_vector(\"great\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.4658365 ,  0.14228567, -0.0147468 , -0.07972424, -0.00482475,\n",
              "        0.17291799, -0.1019064 ,  0.26551396,  0.38831973, -0.01296046,\n",
              "       -0.24330784,  0.35275698, -0.18760549,  0.1195251 ,  0.0108976 ,\n",
              "       -0.02055119,  0.18210387, -0.24476875,  0.32765675,  0.00889295,\n",
              "       -0.2347713 ,  0.16657634,  0.13823693, -0.24581313,  0.20579338,\n",
              "        0.36120856, -0.12453888, -0.04237   , -0.18034935,  0.28278336,\n",
              "        0.1011708 ,  0.07667088, -0.21272978,  0.08931043, -0.0622963 ,\n",
              "       -0.04397372,  0.2651232 , -0.22436944, -0.5980655 ,  0.06624931,\n",
              "       -0.43059924,  0.18476237,  0.4062801 ,  0.476247  ,  0.21408293,\n",
              "       -0.00188831,  0.17435957,  0.01167148, -0.12818441, -0.05374854,\n",
              "       -0.17222664,  0.05946076, -0.2717286 ,  0.18880273, -0.0033478 ,\n",
              "       -0.300269  , -0.0093135 ,  0.07271611,  0.00664317,  0.5401071 ,\n",
              "       -0.11126462,  0.31273788,  0.16003864, -0.10227244,  0.0175306 ,\n",
              "       -0.3204592 , -0.37458053,  0.15087034,  0.22499208, -0.20022649,\n",
              "       -0.4011713 ,  0.18480784, -0.00978556, -0.4203974 ,  0.14803143,\n",
              "       -0.15188712,  0.11426036, -0.06517863,  0.2503933 ,  0.24712151,\n",
              "       -0.19030173,  0.1281908 , -0.29707643, -0.40853962,  0.26566386,\n",
              "        0.12846316,  0.05489708,  0.5034805 ,  0.23736203,  0.02436629,\n",
              "        0.19543901,  0.3159085 ,  0.21525167, -0.5148902 , -0.44584122,\n",
              "       -0.56016207, -0.33570397, -0.28450584, -0.09076874, -0.04542759],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UDd4qkaztRK",
        "colab_type": "code",
        "outputId": "35940753-eafe-45a9-ee7f-8da0bb500c65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "source": [
        "model.get_nearest_neighbors('food')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6cd69932319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_nearest_neighbors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'food'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOnmpEU1zzwL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}