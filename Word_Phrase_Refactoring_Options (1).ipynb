{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Word_Phrase_Refactoring_Options.ipynb","provenance":[{"file_id":"1DD3AXHSUfYHcuFVDBVXjWsgL4Xwu7WBo","timestamp":1577415790084}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"nhkPhiahxg3X","colab_type":"code","colab":{}},"source":["# !pip install scattertext\n","# !pip install -U spacy\n","# !python -m spacy download en_core_web_sm"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CHqfxgryxlCM","colab_type":"code","outputId":"733180e7-7fe0-4901-db04-0ee55c104c64","executionInfo":{"status":"ok","timestamp":1577991938445,"user_tz":300,"elapsed":3019,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":113}},"source":["# from flask import Flask, render_template, request, jsonify\n","import json\n","import warnings\n","import pandas as pd\n","import spacy\n","import scattertext as st\n","from lxml import html\n","from requests import Session\n","from concurrent.futures import ThreadPoolExecutor as Executor\n","import requests\n","# from flask_cors import CORS\n","# from decouple import config\n","import re\n","pd.options.display.max_rows = 999\n","pd.options.display.max_columns = 999\n","pd.set_option('display.max_colwidth', 1000)\n","nlp = spacy.load(\"en_core_web_sm\")#if you run into problems here, 'Restart Runtime' and run all, it might fix things.\n","base_url = \"https://www.yelp.com/biz/\" \n","api_url = \"/review_feed?sort_by=date_desc&start=\"\n","bid = 'Rc1lxc5lSKJYd162JHNMfQ'\n","\n","class Scraper():\n","    def __init__(self):\n","        self.data = pd.DataFrame()\n","\n","    def get_data(self, n, bid=bid):\n","        with Session() as s:\n","            with s.get(base_url+bid+api_url+str(n*20)) as resp: #makes an http get request to given url and returns response as json\n","                r = json.loads(resp.content) #converts json response into a dictionary\n","                _html = html.fromstring(r['review_list']) #loads from dictionary\n","\n","                dates = _html.xpath(\"//div[@class='review-content']/descendant::span[@class='rating-qualifier']/text()\")\n","                reviews = [el.text for el in _html.xpath(\"//div[@class='review-content']/p\")]\n","                ratings = _html.xpath(\"//div[@class='review-content']/descendant::div[@class='biz-rating__stars']/div/@title\")\n","\n","                df = pd.DataFrame([dates, reviews, ratings]).T\n","\n","                self.data = pd.concat([self.data,df])\n","\n","    def scrape(self): #makes it faster\n","        # multithreaded looping\n","        with Executor(max_workers=40) as e:\n","            list(e.map(self.get_data, range(10)))\n","\n","s = Scraper()\n","s.scrape()\n","df = s.data\n","\n","\n","def customtokensize(text):\n","    return re.findall(\"[\\w']+\", str(text))\n","\n","df['tokenized_text'] = df[1].apply(customtokensize)\n","# stopwords = ['and','was','were','had','check-in','=','= =','u','want', 'u want', 'cuz','him',\"i've\",'on', 'her','told','ins', '1 check','I', 'i\"m', 'i', ' ', 'it', \"it's\", 'it.','they', 'the', 'this','its', 'l','they','this',\"don't\",'the ', ' the', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.',',']\n","\n","stopwords = [',','\"','!','-','&','?']\n","\n","def filter_stopwords(text):\n","  nonstopwords = []\n","  for i in text:\n","    if i not in stopwords:\n","      nonstopwords.append(i)\n","  return nonstopwords\n","df['tokenized_text'] = df['tokenized_text'].apply(filter_stopwords)\n","df['parts_of_speech_reference'] = df['tokenized_text'].apply(filter_stopwords)\n","df['parts_of_speech_reference'] = df['parts_of_speech_reference'].str.join(' ')\n","# df['parts_of_speech_reference'] = df['tokenized_text'].str.join(' ')\n","df.head(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>tokenized_text</th>\n","      <th>parts_of_speech_reference</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\n        12/12/2019\\n</td>\n","      <td>I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.</td>\n","      <td>4.0 star rating</td>\n","      <td>[I, used, to, come, here, for, dinner, when, I, was, traveling, for, work, Now, I, come, here, for, brunch, before, Broadway, matinees]</td>\n","      <td>I used to come here for dinner when I was traveling for work Now I come here for brunch before Broadway matinees</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            0  \\\n","0  \\n        12/12/2019\\n       \n","\n","                                                                                                                    1  \\\n","0  I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.   \n","\n","                 2  \\\n","0  4.0 star rating   \n","\n","                                                                                                                            tokenized_text  \\\n","0  [I, used, to, come, here, for, dinner, when, I, was, traveling, for, work, Now, I, come, here, for, brunch, before, Broadway, matinees]   \n","\n","                                                                                          parts_of_speech_reference  \n","0  I used to come here for dinner when I was traveling for work Now I come here for brunch before Broadway matinees  "]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"wc4BuxPCNOu1","colab_type":"code","colab":{}},"source":["# def find_nounchunks(x):\n","#   \"\"\"noun chunks greater than 2 without stopwords\"\"\"\n","#   noun_list = []\n","#   doc = nlp(str(x))\n","#   for chunk in doc.noun_chunks:\n","#     if len(chunk) > 2:\n","#       noun_list.append(chunk)\n","#   return noun_list\n","\n","# df['nounchunk_list'] = df[1].apply(find_nounchunks)\n","# df[[1,'nounchunk_list']].head(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Iw8izWr5QIa7","colab_type":"code","colab":{}},"source":["# def find_words_after_and_before_THE_A_AN(x):\n","#   \"\"\"find_words_after_and_before_THE\"\"\"\n","#   total_lists = []\n","#   doc = nlp(str(x))\n","#   for token in range(len(doc)):\n","#     try:\n","#       word_list1 = []\n","#       if (doc[token].pos_ == 'PRON') or ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An'):\n","#         word_list1.append(doc[token])\n","#         word_list1.append(doc[token+1])\n","#         word_list1.append(doc[token+2])\n","#         word_list1.append(doc[token+3])\n","#         word_list1.append(doc[token+4])\n","#         word_list1.append(doc[token+5])\n","#         word_list1.append(doc[token+6])\n","#         word_list1.append(doc[token+7])\n","#         word_list1.append(doc[token+8])\n","#         word_list1.append(doc[token+9])\n","#         word_list1.append(doc[token+10])\n","#         word_list1.append(doc[token+11])\n","#       if len(word_list1) != 0:\n","#         total_lists.append(word_list1)\n","#     except IndexError or TypeError as e:\n","#       for token in range(len(doc)):\n","#         try:\n","#           word_list2 = []\n","#           if (doc[token].pos_ == 'PRON') or ((doc[token].pos_ == 'the') and (doc[token].pos_ == 'PRON')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An'):\n","#             word_list2.append(doc[token])\n","#             word_list2.append(doc[token+1])\n","#             word_list2.append(doc[token+2])\n","#             word_list2.append(doc[token+3])\n","#             word_list2.append(doc[token+4])\n","#             word_list2.append(doc[token+5])\n","#             word_list2.append(doc[token+6])\n","#             word_list2.append(doc[token+7])\n","#           if (len(word_list2) != 0) and (False for i in word_list2 for i in word_list1) and (False for i in word_list2 for i in total_lists):\n","#             total_lists.append(word_list2) \n","#         except IndexError or TypeError as e:\n","#           for token in range(len(doc)):\n","#             try:\n","#               word_list3 = []\n","#               if ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')):\n","#                 word_list3.append(doc[token-1])\n","#                 word_list3.append(doc[token])\n","#                 word_list3.append(doc[token+1])\n","#                 word_list3.append(doc[token+2])\n","#               if (len(word_list3) != 0 ) and (False for i in word_list3 for i in word_list2) and (False for i in word_list3 for i in total_lists):\n","#                 total_lists.append(word_list3)\n","#             except IndexError or TypeError as e:\n","#               for token in range(len(doc)):\n","#                 try:\n","#                   word_list4 = []\n","#                   if (doc[token].text == 'the') or (doc[token].text == 'a') or (doc[token].text == 'an'):\n","#                     word_list4.append(doc[token])\n","#                     word_list4.append(doc[token+1])\n","#                     word_list4.append(doc[token+2])\n","#                     word_list4.append(doc[token+3])\n","#                   if (len(word_list4) != 0 ) and (False for i in word_list4 for i in word_list3) and (False for i in word_list4 for i in total_lists):\n","#                     total_lists.append(word_list4)\n","#                 except IndexError or TypeError as e:\n","#                   pass\n","#   return total_lists\n","\n","# df['words_around_THE_A_AN'] = df['parts_of_speech_reference'].apply(find_words_after_and_before_THE_A_AN)\n","# df[[1,'words_around_THE_A_AN']].head(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PpWvw8GwyC1B","colab_type":"code","colab":{}},"source":["# def find_phrases(x):\n","#   long_phrase_list = []\n","#   doc = nlp(str(x))\n","#   for token in range(len(doc)):\n","#     sub_list = []\n","#     try:\n","#       if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","#         if doc[token-1] not in sub_list and doc[token] not in sub_list and doc[token+1] not in sub_list and doc[token+2] not in sub_list and doc[token+3] not in sub_list:\n","#           sub_list.append(doc[token-1])\n","#           sub_list.append(doc[token])\n","#           sub_list.append(doc[token+1])\n","#           sub_list.append(doc[token+2])\n","#           sub_list.append(doc[token+3])\n","#     except IndexError as e:\n","#       try:\n","#         if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","#           if doc[token-1] not in sub_list and doc[token] not in sub_list and doc[token+1] not in sub_list and doc[token+2] not in sub_list:\n","#             sub_list.append(doc[token-1])\n","#             sub_list.append(doc[token])\n","#             sub_list.append(doc[token+1])\n","#             sub_list.append(doc[token+2])\n","#       except IndexError as e:\n","#         try:\n","#           if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","#             if doc[token-1] not in sub_list and doc[token] not in sub_list and doc[token+1] not in sub_list:\n","#               sub_list.append(doc[token-1])\n","#               sub_list.append(doc[token])\n","#               sub_list.append(doc[token+1])\n","#         except IndexError as e:\n","#           try:\n","#             if (doc[token].text == 'on') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","#               if doc[token] not in sub_list and doc[token+1] not in sub_list:\n","#                 sub_list.append(doc[token])\n","#                 sub_list.append(doc[token+1])\n","#           except IndexError as e:\n","#             try:\n","#               #captures words at the end of reviews\n","#               if (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token-1].pos_ == 'ADJ'and doc[token].pos_ =='NOUN') or (doc[token-1].pos_ == 'VERB'and doc[token].pos_ =='NOUN')or (doc[token-1].pos_ == 'NOUN'and doc[token].pos_ =='NOUN'):\n","#                 if doc[token] not in sub_list and doc[token+1] not in sub_list:\n","#                   sub_list.append(doc[token-1])\n","#                   sub_list.append(doc[token])\n","#             except IndexError as e:\n","#               try:\n","#                 #captures words from the start of reviews\n","#                 if (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'NOUN' and doc[token+1].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN')or (doc[token].pos_ == 'NOUN'and doc[token+1].pos_ =='NOUN'):\n","#                   if doc[token] not in sub_list and doc[token+1] not in sub_list:\n","#                     sub_list.append(doc[token])\n","#                     sub_list.append(doc[token+1])\n","#               except IndexError as e:\n","#                 pass\n","#     if (len(sub_list) != 0) and (sub_list not in long_phrase_list):\n","#       long_phrase_list.append(sub_list)\n","#   return long_phrase_list\n","\n","\n","# df['AdjNouns_NounNouns_Service_Food_For'] = df[1].apply(find_phrases)\n","# df[[1,'nounchunk_list','AdjNouns_NounNouns_Service_Food_For','words_around_THE_A_AN']].sample(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZDSdPg-QymZ7","colab_type":"code","colab":{}},"source":["# def find_word_segments(x):\n","#   long_phrase_list = []\n","#   doc = nlp(str(x))\n","#   try:\n","#     for token in range(len(doc)):\n","#       sub_list = []\n","#       if (token < (int(len(doc))-5)) and ((doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN') or (doc[token].text == 'food') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN')):\n","#         sub_list.append(doc[token])\n","#         sub_list.append(doc[token+1])\n","#         sub_list.append(doc[token+2])\n","#         sub_list.append(doc[token+3])\n","#         sub_list.append(doc[token+4])\n","#         sub_list.append(doc[token+5])\n","#         # sub_list.append(doc[token+6])\n","#       if len(sub_list) != 0:\n","#         long_phrase_list.append(sub_list)\n","#   except IndexError as e:\n","#     pass\n","#   try:\n","#     for token in range(len(doc)):\n","#       sub_list = []\n","#       if (token < (int(len(doc))-5)) and ((doc[token].pos_ == 'PRON') or ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An')):\n","#         sub_list.append(doc[token])\n","#         sub_list.append(doc[token+1])\n","#         sub_list.append(doc[token+2])\n","#         sub_list.append(doc[token+3])\n","#         sub_list.append(doc[token+4])\n","#         sub_list.append(doc[token+5])\n","#       if len(sub_list) != 0:\n","#         long_phrase_list.append(sub_list)\n","#   except IndexError as e:\n","#     pass\n","#   return long_phrase_list\n","\n","# df['word_segments'] = df[1].apply(find_word_segments)\n","# df[[1,'word_segments']].head(1)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NTdHbBe2Dn_2","colab_type":"code","outputId":"8c37e3e6-7131-457c-ed92-2b7580bc5287","executionInfo":{"status":"ok","timestamp":1577991952469,"user_tz":300,"elapsed":3866,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":225}},"source":["def find_word_segment_combined(x):\n","  long_phrase_list = []\n","  doc = nlp(str(x))\n","  try:\n","    for token in range(len(doc)):\n","      sub_list = []\n","      if (token < (int(len(doc))-5)) and ((doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN') or (doc[token].text == 'food') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN')):\n","        sub_list.append(doc[token])\n","        sub_list.append(doc[token+1])\n","        sub_list.append(doc[token+2])\n","        sub_list.append(doc[token+3])\n","        sub_list.append(doc[token+4])\n","        sub_list.append(doc[token+5])\n","        sub_list.append(doc[token+6])\n","      if len(sub_list) != 0:\n","        long_phrase_list.append(sub_list)\n","  except IndexError as e:\n","    try:\n","      for token in range(len(doc)):\n","        sub_list = []\n","        if (token < (int(len(doc))-5)) and ((doc[token].pos_ == 'PRON') or ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An')):\n","          sub_list.append(doc[token])\n","          sub_list.append(doc[token+1])\n","          sub_list.append(doc[token+2])\n","          sub_list.append(doc[token+3])\n","          sub_list.append(doc[token+4])\n","          sub_list.append(doc[token+5])\n","        if len(sub_list) != 0:\n","          long_phrase_list.append(sub_list)\n","    except IndexError as e:\n","      try:\n","        word_list1 = []\n","        if (doc[token].pos_ == 'PRON') or ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An'):\n","          word_list1.append(doc[token])\n","          word_list1.append(doc[token+1])\n","          word_list1.append(doc[token+2])\n","          word_list1.append(doc[token+3])\n","          word_list1.append(doc[token+4])\n","          word_list1.append(doc[token+5])\n","          word_list1.append(doc[token+6])\n","          word_list1.append(doc[token+7])\n","          word_list1.append(doc[token+8])\n","          word_list1.append(doc[token+9])\n","          word_list1.append(doc[token+10])\n","          word_list1.append(doc[token+11])\n","        if len(word_list1) != 0:\n","          long_phrase_list.append(word_list1)\n","      except IndexError or TypeError as e:\n","        for token in range(len(doc)):\n","          try:\n","            word_list2 = []\n","            if (doc[token].pos_ == 'PRON') or ((doc[token].pos_ == 'the') and (doc[token].pos_ == 'PRON')) or (doc[token].text == 'The') or (doc[token].text == 'a') or (doc[token].text == 'A') or (doc[token].text == 'an') or (doc[token].text == 'An'):\n","              word_list2.append(doc[token])\n","              word_list2.append(doc[token+1])\n","              word_list2.append(doc[token+2])\n","              word_list2.append(doc[token+3])\n","              word_list2.append(doc[token+4])\n","              word_list2.append(doc[token+5])\n","              word_list2.append(doc[token+6])\n","              word_list2.append(doc[token+7])\n","            if (len(word_list2) != 0) and (False for i in word_list2 for i in word_list1) and (False for i in word_list2 for i in long_phrase_list):\n","              long_phrase_list.append(word_list2) \n","          except IndexError or TypeError as e:\n","            for token in range(len(doc)):\n","              try:\n","                word_list3 = []\n","                if ((doc[token-1].pos_ == 'VERB') and (doc[token].text == 'the')) or ((doc[token-1].pos_ == 'ADV') and (doc[token].text == 'the')):\n","                  word_list3.append(doc[token-1])\n","                  word_list3.append(doc[token])\n","                  word_list3.append(doc[token+1])\n","                  word_list3.append(doc[token+2])\n","                if (len(word_list3) != 0 ) and (False for i in word_list3 for i in word_list2) and (False for i in word_list3 for i in long_phrase_list):\n","                  long_phrase_list.append(word_list3)\n","              except IndexError or TypeError as e:\n","                for token in range(len(doc)):\n","                  try:\n","                    word_list4 = []\n","                    if (doc[token].text == 'the') or (doc[token].text == 'a') or (doc[token].text == 'an'):\n","                      word_list4.append(doc[token])\n","                      word_list4.append(doc[token+1])\n","                      word_list4.append(doc[token+2])\n","                      word_list4.append(doc[token+3])\n","                    if (len(word_list4) != 0 ) and (False for i in word_list4 for i in word_list3) and (False for i in word_list4 for i in long_phrase_list):\n","                      long_phrase_list.append(word_list4)\n","                  except IndexError or TypeError as e:\n","                    try:\n","                      word_list5 = []\n","                      if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","                        word_list5.append(doc[token-1])\n","                        word_list5.append(doc[token])\n","                        word_list5.append(doc[token+1])\n","                        word_list5.append(doc[token+2])\n","                        word_list5.append(doc[token+3])\n","                      if (len(word_list5) != 0) and (False for i in word_list5 for i in word_list4) and (False for i in word_list5 for i in long_phrase_list):\n","                        long_phrase_list.append(word_list5) \n","                    except IndexError as e:\n","                      try:\n","                        word_list6 = []\n","                        if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","                          word_list6.append(doc[token-1])\n","                          word_list6.append(doc[token])\n","                          word_list6.append(doc[token+1])\n","                          word_list6.append(doc[token+2])\n","                        if (len(word_list6) != 0) and (False for i in word_list6 for i in word_list5) and (False for i in word_list6 for i in long_phrase_list):\n","                          long_phrase_list.append(word_list6) \n","                      except IndexError as e:\n","                        try:\n","                          word_list7 = []\n","                          if (doc[token-1].text == 'on') and (doc[token].text == 'a') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","                            word_list7.append(doc[token-1])\n","                            word_list7.append(doc[token])\n","                            word_list7.append(doc[token+1])\n","                          if (len(word_list7) != 0) and (False for i in word_list7 for i in word_list6) and (False for i in word_list7 for i in long_phrase_list):\n","                            long_phrase_list.append(word_list7) \n","                        except IndexError as e:\n","                          try:\n","                            word_list8 = []\n","                            if (doc[token].text == 'on') or (doc[token].text == 'for') or (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'VERB' and doc[token+1].pos_ == 'NOUN') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'ADJ'and doc[token+2].pos_ =='NOUN')or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='VERB'and doc[token+2].pos_ =='NOUN') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ == 'NOUN'and doc[token+2].pos_ =='NOUN'):\n","                              word_list8.append(doc[token])\n","                              word_list8.append(doc[token+1])\n","                            if (len(word_list8) != 0) and (False for i in word_list8 for i in word_list7) and (False for i in word_list8 for i in long_phrase_list):\n","                              long_phrase_list.append(word_list8) \n","                          except IndexError as e:\n","                            try:\n","                              word_list9 = []\n","                              #captures words at the end of reviews\n","                              if (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token-1].pos_ == 'NOUN' and doc[token].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token-1].pos_ == 'ADJ'and doc[token].pos_ =='NOUN') or (doc[token-1].pos_ == 'VERB'and doc[token].pos_ =='NOUN')or (doc[token-1].pos_ == 'NOUN'and doc[token].pos_ =='NOUN'):\n","                                word_list9.append(doc[token-1])\n","                                word_list9.append(doc[token])\n","                              if (len(word_list9) != 0) and (False for i in word_list9 for i in word_list8) and (False for i in word_list9 for i in long_phrase_list):\n","                                long_phrase_list.append(word_list9) \n","                            except IndexError as e:\n","                              try:\n","                                word_list10 = []\n","                                #captures words from the start of reviews\n","                                if (doc[token].text == 'food is') or (doc[token].lemma_ == 'order') or (doc[token].text == 'but') or (doc[token].text == 'not') or (doc[token].pos_ == 'NOUN' and doc[token+1].pos_ == 'VERB') or (doc[token].lemma_ == 'service') or (doc[token].lemma_ == 'wait') or (doc[token].pos_ == 'ADJ'and doc[token+1].pos_ =='NOUN') or (doc[token].pos_ == 'VERB'and doc[token+1].pos_ =='NOUN')or (doc[token].pos_ == 'NOUN'and doc[token+1].pos_ =='NOUN'):\n","                                  word_list10.append(doc[token])\n","                                  word_list10.append(doc[token+1])\n","                                if (len(word_list10) != 0) and (False for i in word_list10 for i in word_list9) and (False for i in word_list10 for i in long_phrase_list):\n","                                  long_phrase_list.append(word_list10) \n","                              except IndexError as e:\n","                                pass\n","  return long_phrase_list\n","\n","df['word_segments'] = df[1].apply(find_word_segment_combined)\n","df[[1,'word_segments']].head(3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>1</th>\n","      <th>word_segments</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Great for girls night, our waiter was amazing. Great at food and drink recommendations. The ricotta was delicious! Good dinner, however pricy!</td>\n","      <td>[[food, and, drink, recommendations, ., The, ricotta], [drink, recommendations, ., The, ricotta, was, delicious], [The, ricotta, was, delicious, !, Good]]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>I am an Italian food fanatic, so once I saw the beautiful decor and delicious menu, I figured I had to visit. The tomato soup was quite delicious, even though the thickness was more alike to a pasta sauce as it was also filled with cheese. For my main course, I ordered the Salmon. BEST DECISION EVER. Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor. For dessert, I would definitely recommend the Lemon Custard Tart!</td>\n","      <td>[[Italian, food, fanatic, ,, so, once, I], [food, fanatic, ,, so, once, I, saw], [delicious, menu, ,, I, figured, I, had], [main, course, ,, I, ordered, the, Salmon], [ordered, the, Salmon, ., BEST, DECISION, EVER], [Other, members, in, my, party, ordered, the], [ordered, the, spinach, pasta, and, the, gnocchi], [not, match, up, to, the, salmon, which]]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1  \\\n","0                                                                                                                                                                                                                                                                                                                                                                                                                  I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.   \n","1                                                                                                                                                                                                                                                                                                                                                                                      Great for girls night, our waiter was amazing. Great at food and drink recommendations. The ricotta was delicious! Good dinner, however pricy!   \n","2  I am an Italian food fanatic, so once I saw the beautiful decor and delicious menu, I figured I had to visit. The tomato soup was quite delicious, even though the thickness was more alike to a pasta sauce as it was also filled with cheese. For my main course, I ordered the Salmon. BEST DECISION EVER. Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor. For dessert, I would definitely recommend the Lemon Custard Tart!   \n","\n","                                                                                                                                                                                                                                                                                                                                                         word_segments  \n","0                                                                                                                                                                                                                                                                                                                                                                   []  \n","1                                                                                                                                                                                                           [[food, and, drink, recommendations, ., The, ricotta], [drink, recommendations, ., The, ricotta, was, delicious], [The, ricotta, was, delicious, !, Good]]  \n","2  [[Italian, food, fanatic, ,, so, once, I], [food, fanatic, ,, so, once, I, saw], [delicious, menu, ,, I, figured, I, had], [main, course, ,, I, ordered, the, Salmon], [ordered, the, Salmon, ., BEST, DECISION, EVER], [Other, members, in, my, party, ordered, the], [ordered, the, spinach, pasta, and, the, gnocchi], [not, match, up, to, the, salmon, which]]  "]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"RisUp0zKwaIJ","colab_type":"code","outputId":"53c62a40-29b5-4746-e28b-b459900db100","executionInfo":{"status":"ok","timestamp":1577991953969,"user_tz":300,"elapsed":527,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":529}},"source":["df['word_segments_unpacked'] = df['word_segments'].apply(lambda l: [item for sublist in l for item in sublist])\n","df['word_segments_unpacked'] = df['word_segments_unpacked'].astype(str)\n","df['word_segments_unpacked'] = df['word_segments_unpacked'].apply(lambda x: ''.join([str(i) for i in x]))\n","df.head(3)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>tokenized_text</th>\n","      <th>parts_of_speech_reference</th>\n","      <th>word_segments</th>\n","      <th>word_segments_unpacked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>\\n        12/12/2019\\n</td>\n","      <td>I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.</td>\n","      <td>4.0 star rating</td>\n","      <td>[I, used, to, come, here, for, dinner, when, I, was, traveling, for, work, Now, I, come, here, for, brunch, before, Broadway, matinees]</td>\n","      <td>I used to come here for dinner when I was traveling for work Now I come here for brunch before Broadway matinees</td>\n","      <td>[]</td>\n","      <td>[]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>\\n        12/11/2019\\n</td>\n","      <td>Great for girls night, our waiter was amazing. Great at food and drink recommendations. The ricotta was delicious! Good dinner, however pricy!</td>\n","      <td>4.0 star rating</td>\n","      <td>[Great, for, girls, night, our, waiter, was, amazing, Great, at, food, and, drink, recommendations, The, ricotta, was, delicious, Good, dinner, however, pricy]</td>\n","      <td>Great for girls night our waiter was amazing Great at food and drink recommendations The ricotta was delicious Good dinner however pricy</td>\n","      <td>[[food, and, drink, recommendations, ., The, ricotta], [drink, recommendations, ., The, ricotta, was, delicious], [The, ricotta, was, delicious, !, Good]]</td>\n","      <td>[food, and, drink, recommendations, ., The, ricotta, drink, recommendations, ., The, ricotta, was, delicious, The, ricotta, was, delicious, !, Good]</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>\\n        12/11/2019\\n</td>\n","      <td>I am an Italian food fanatic, so once I saw the beautiful decor and delicious menu, I figured I had to visit. The tomato soup was quite delicious, even though the thickness was more alike to a pasta sauce as it was also filled with cheese. For my main course, I ordered the Salmon. BEST DECISION EVER. Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor. For dessert, I would definitely recommend the Lemon Custard Tart!</td>\n","      <td>4.0 star rating</td>\n","      <td>[I, am, an, Italian, food, fanatic, so, once, I, saw, the, beautiful, decor, and, delicious, menu, I, figured, I, had, to, visit, The, tomato, soup, was, quite, delicious, even, though, the, thickness, was, more, alike, to, a, pasta, sauce, as, it, was, also, filled, with, cheese, For, my, main, course, I, ordered, the, Salmon, BEST, DECISION, EVER, Other, members, in, my, party, ordered, the, spinach, pasta, and, the, gnocchi, and, they, just, simply, did, not, match, up, to, the, salmon, which, bursted, with, flavor, For, dessert, I, would, definitely, recommend, the, Lemon, Custard, Tart]</td>\n","      <td>I am an Italian food fanatic so once I saw the beautiful decor and delicious menu I figured I had to visit The tomato soup was quite delicious even though the thickness was more alike to a pasta sauce as it was also filled with cheese For my main course I ordered the Salmon BEST DECISION EVER Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor For dessert I would definitely recommend the Lemon Custard Tart</td>\n","      <td>[[Italian, food, fanatic, ,, so, once, I], [food, fanatic, ,, so, once, I, saw], [delicious, menu, ,, I, figured, I, had], [main, course, ,, I, ordered, the, Salmon], [ordered, the, Salmon, ., BEST, DECISION, EVER], [Other, members, in, my, party, ordered, the], [ordered, the, spinach, pasta, and, the, gnocchi], [not, match, up, to, the, salmon, which]]</td>\n","      <td>[Italian, food, fanatic, ,, so, once, I, food, fanatic, ,, so, once, I, saw, delicious, menu, ,, I, figured, I, had, main, course, ,, I, ordered, the, Salmon, ordered, the, Salmon, ., BEST, DECISION, EVER, Other, members, in, my, party, ordered, the, ordered, the, spinach, pasta, and, the, gnocchi, not, match, up, to, the, salmon, which]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            0  \\\n","0  \\n        12/12/2019\\n       \n","1  \\n        12/11/2019\\n       \n","2  \\n        12/11/2019\\n       \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    1  \\\n","0                                                                                                                                                                                                                                                                                                                                                                                                                  I used to come here for dinner when I was traveling for work. Now I come here for brunch before Broadway matinees.   \n","1                                                                                                                                                                                                                                                                                                                                                                                      Great for girls night, our waiter was amazing. Great at food and drink recommendations. The ricotta was delicious! Good dinner, however pricy!   \n","2  I am an Italian food fanatic, so once I saw the beautiful decor and delicious menu, I figured I had to visit. The tomato soup was quite delicious, even though the thickness was more alike to a pasta sauce as it was also filled with cheese. For my main course, I ordered the Salmon. BEST DECISION EVER. Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor. For dessert, I would definitely recommend the Lemon Custard Tart!   \n","\n","                 2  \\\n","0  4.0 star rating   \n","1  4.0 star rating   \n","2  4.0 star rating   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           tokenized_text  \\\n","0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 [I, used, to, come, here, for, dinner, when, I, was, traveling, for, work, Now, I, come, here, for, brunch, before, Broadway, matinees]   \n","1                                                                                                                                                                                                                                                                                                                                                                                                                                                         [Great, for, girls, night, our, waiter, was, amazing, Great, at, food, and, drink, recommendations, The, ricotta, was, delicious, Good, dinner, however, pricy]   \n","2  [I, am, an, Italian, food, fanatic, so, once, I, saw, the, beautiful, decor, and, delicious, menu, I, figured, I, had, to, visit, The, tomato, soup, was, quite, delicious, even, though, the, thickness, was, more, alike, to, a, pasta, sauce, as, it, was, also, filled, with, cheese, For, my, main, course, I, ordered, the, Salmon, BEST, DECISION, EVER, Other, members, in, my, party, ordered, the, spinach, pasta, and, the, gnocchi, and, they, just, simply, did, not, match, up, to, the, salmon, which, bursted, with, flavor, For, dessert, I, would, definitely, recommend, the, Lemon, Custard, Tart]   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 parts_of_speech_reference  \\\n","0                                                                                                                                                                                                                                                                                                                                                                                                         I used to come here for dinner when I was traveling for work Now I come here for brunch before Broadway matinees   \n","1                                                                                                                                                                                                                                                                                                                                                                                 Great for girls night our waiter was amazing Great at food and drink recommendations The ricotta was delicious Good dinner however pricy   \n","2  I am an Italian food fanatic so once I saw the beautiful decor and delicious menu I figured I had to visit The tomato soup was quite delicious even though the thickness was more alike to a pasta sauce as it was also filled with cheese For my main course I ordered the Salmon BEST DECISION EVER Other members in my party ordered the spinach pasta and the gnocchi and they just simply did not match up to the salmon which bursted with flavor For dessert I would definitely recommend the Lemon Custard Tart   \n","\n","                                                                                                                                                                                                                                                                                                                                                         word_segments  \\\n","0                                                                                                                                                                                                                                                                                                                                                                   []   \n","1                                                                                                                                                                                                           [[food, and, drink, recommendations, ., The, ricotta], [drink, recommendations, ., The, ricotta, was, delicious], [The, ricotta, was, delicious, !, Good]]   \n","2  [[Italian, food, fanatic, ,, so, once, I], [food, fanatic, ,, so, once, I, saw], [delicious, menu, ,, I, figured, I, had], [main, course, ,, I, ordered, the, Salmon], [ordered, the, Salmon, ., BEST, DECISION, EVER], [Other, members, in, my, party, ordered, the], [ordered, the, spinach, pasta, and, the, gnocchi], [not, match, up, to, the, salmon, which]]   \n","\n","                                                                                                                                                                                                                                                                                                                                word_segments_unpacked  \n","0                                                                                                                                                                                                                                                                                                                                                   []  \n","1                                                                                                                                                                                                 [food, and, drink, recommendations, ., The, ricotta, drink, recommendations, ., The, ricotta, was, delicious, The, ricotta, was, delicious, !, Good]  \n","2  [Italian, food, fanatic, ,, so, once, I, food, fanatic, ,, so, once, I, saw, delicious, menu, ,, I, figured, I, had, main, course, ,, I, ordered, the, Salmon, ordered, the, Salmon, ., BEST, DECISION, EVER, Other, members, in, my, party, ordered, the, ordered, the, spinach, pasta, and, the, gnocchi, not, match, up, to, the, salmon, which]  "]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"oKk1cDQMz-TM","colab_type":"code","outputId":"389d09e5-5819-451b-c248-23e225206e05","executionInfo":{"status":"ok","timestamp":1577991955161,"user_tz":300,"elapsed":486,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":79}},"source":["phrase_count = df[['word_segments_unpacked', 2]]\n","import numpy as np\n","\n","s= phrase_count.apply(lambda x: pd.Series(x['word_segments_unpacked']),axis=1).stack().reset_index(level=1, drop=True)\n","s.name = 'word_segments_unpacked'\n","\n","phrase_count = phrase_count.drop('word_segments_unpacked', axis=1).join(s)\n","phrase_count.head(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>2</th>\n","      <th>word_segments_unpacked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>4.0 star rating</td>\n","      <td>[]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                 2 word_segments_unpacked\n","0  4.0 star rating                     []"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"EYL09rUh23Xc","colab_type":"code","outputId":"09135c4f-5ef0-4c67-da57-3cfd60d5ab57","executionInfo":{"status":"ok","timestamp":1577991956364,"user_tz":300,"elapsed":520,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":79}},"source":["phrase_count = pd.DataFrame(df['word_segments_unpacked'].str.split(',').tolist(), index=df[2]).stack()\n","phrase_count = phrase_count.reset_index()[[0, 2]] # var1 variable is currently labeled 0\n","phrase_count.columns = ['word_segments_unpacked', 'ratings'] # renaming var1\n","phrase_count = phrase_count.reset_index(drop=False)\n","\n","# x = ''\n","# replaceDict = {'to' : '', '!]' : '', '-' : '' }    \n","# stopwords = ['to','We','had','.',\"'re\",'it','or','he','she','we','us','how','went','no',\"'d\",'\"','of','has','a','by','bit','thing','NaN','place',' ','on','so','ok','i','and','they','None','was','you',\"'ve\",'did',\"'d\",'be','and','but','is','as','a','b','c','d','e','f','g','u','it','!','&','you','has','-','None',':','and','had','was','him','so','my',' ','did','would','on','her','him','it','is','by','bit','thing','NaN','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', '1 check', 'i\"m', \"it's\",'they', ' the','the ',' ', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n","# def filter_stopwords(text):\n","#   for i in str(text):\n","#     if i not in stopwords:\n","#       return str(text)\n","\n","# phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].map(filter_stopwords)\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace('[','')\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(']','')\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace('-','')\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace('.','')\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.replace(' ','')\n","phrase_count['word_segments_unpacked'] = phrase_count['word_segments_unpacked'].str.lower()\n","# phrase_count = phrase_count[phrase_count['word_segments_unpacked']!=i for i in stopwords]\n","\n","stopwords = ['0','1','2','3','4','5','6','7','8','9','/','$',\"'d\",\"'ll\",\"'m\",'+','maybe','from','first','here','only','put','where','got','sure','definitely','food','yet','our','go','since','really','very','two',\"n't\",'with','if',\"'s\",'which','came','all','me','(',')','makes','make','were','immediately','get','been','ahead','also','that','one','have','see','what','to','we','had','.',\"'re\",'it','or','he','she','we','us','how','went','no','\"','of','has','by','bit','thing','place','so','ok','and','they','none','was','you',\"'ve\",'did','be','and','but','is','as','&','you','has','-',':','and','had','was','him','so','my','did','would','her','him','it','is','by','bit','thing','place','[',']','while','check-in','=','= =','want', 'good','husband', 'want','love','something','your','they','your','cuz','him',\"i've\",'her','told', 'check', 'i\"m', \"it's\",'they', 'this','its','they','this',\"don't\",'the',',', 'it', 'i\"ve', 'i\"m', '!', '1','2','3','4', '5','6','7','8','9','0','/','.']\n","def filter_stopwords(text):\n","  for i in str(text):\n","    if i not in stopwords:\n","      return str(text)\n","\n","phrase_count = phrase_count[~phrase_count['word_segments_unpacked'].isin(stopwords)]#if item in stopwords list partially matches, delete\n","\n","#full matches\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='i']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='a']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='an']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='am']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='at']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='are']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='in']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='on']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='for']\n","phrase_count = phrase_count[phrase_count['word_segments_unpacked']!='']\n","phrase_count.sample(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>index</th>\n","      <th>word_segments_unpacked</th>\n","      <th>ratings</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1331</th>\n","      <td>1331</td>\n","      <td>fixe</td>\n","      <td>3.0 star rating</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["      index word_segments_unpacked          ratings\n","1331   1331                   fixe  3.0 star rating"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"-0qCSp2S8rtd","colab_type":"code","outputId":"de7e97a4-566d-4a67-dc5e-58d1f5575126","executionInfo":{"status":"ok","timestamp":1577991957627,"user_tz":300,"elapsed":375,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["#test values\n","phrase_count.iloc[85]['word_segments_unpacked']"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'bottomless'"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"Coh1F9H9FGi7","colab_type":"code","colab":{}},"source":["# phrase_count.tail(188)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LA3pWzD209oe","colab_type":"code","outputId":"727d1988-90b8-4141-e622-b113973f676c","executionInfo":{"status":"ok","timestamp":1577991960015,"user_tz":300,"elapsed":545,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":477}},"source":["phrase_count_pivot = pd.pivot_table(phrase_count, index='word_segments_unpacked', columns='ratings', aggfunc='count', fill_value=0)\n","phrase_count_pivot.sort_values(by=('index','1.0 star rating'), ascending=False)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","\n","    .dataframe thead tr:last-of-type th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th colspan=\"5\" halign=\"left\">index</th>\n","    </tr>\n","    <tr>\n","      <th>ratings</th>\n","      <th>1.0 star rating</th>\n","      <th>2.0 star rating</th>\n","      <th>3.0 star rating</th>\n","      <th>4.0 star rating</th>\n","      <th>5.0 star rating</th>\n","    </tr>\n","    <tr>\n","      <th>word_segments_unpacked</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>service</th>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>30</td>\n","      <td>41</td>\n","    </tr>\n","    <tr>\n","      <th>wait</th>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>order</th>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>lukewarm</th>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>not</th>\n","      <td>4</td>\n","      <td>21</td>\n","      <td>23</td>\n","      <td>16</td>\n","      <td>13</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>fresh</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>friend</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>friendly</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>friends</th>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1069 rows × 5 columns</p>\n","</div>"],"text/plain":["                                 index                                  \\\n","ratings                1.0 star rating 2.0 star rating 3.0 star rating   \n","word_segments_unpacked                                                   \n","service                             13              11              13   \n","wait                                 8               1               1   \n","order                                6               3               1   \n","lukewarm                             4               0               0   \n","not                                  4              21              23   \n","...                                ...             ...             ...   \n","fresh                                0               0               3   \n","friend                               0               0               0   \n","friendly                             0               0               1   \n","friends                              0               0               1   \n","                                     0               1               0   \n","\n","                                                        \n","ratings                4.0 star rating 5.0 star rating  \n","word_segments_unpacked                                  \n","service                             30              41  \n","wait                                 3               4  \n","order                                5               7  \n","lukewarm                             0               0  \n","not                                 16              13  \n","...                                ...             ...  \n","fresh                                1               2  \n","friend                               1               4  \n","friendly                             6               4  \n","friends                              4               2  \n","                                     0               0  \n","\n","[1069 rows x 5 columns]"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"vQxOO48GtZWR","colab_type":"code","colab":{}},"source":["phrase_count_pivot.columns = [''.join(col).strip() for col in phrase_count_pivot.columns.values]#flatten index levels part 1\n","phrase_count_pivot = pd.DataFrame(phrase_count_pivot.to_records())#flatten index levels part 2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6hTG-klrsaP9","colab_type":"code","outputId":"24725c1c-16a5-47d4-d536-54977b064c4f","executionInfo":{"status":"ok","timestamp":1577991962879,"user_tz":300,"elapsed":490,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":415}},"source":["\n","#not necessary\n","# phrase_count_pivot['index1.0 star rating'] = phrase_count_pivot['index1.0 star rating']*(-2)\n","# phrase_count_pivot['index2.0 star rating'] = phrase_count_pivot['index2.0 star rating']*(-1)\n","# phrase_count_pivot['index3.0 star rating'] = phrase_count_pivot['index3.0 star rating']*(-0.1)\n","# phrase_count_pivot['index4.0 star rating'] = phrase_count_pivot['index4.0 star rating']*(1)\n","# phrase_count_pivot['index5.0 star rating'] = phrase_count_pivot['index5.0 star rating']*(2)\n","\n","phrase_count_pivot['score'] = phrase_count_pivot['index1.0 star rating'] + phrase_count_pivot['index2.0 star rating'] + phrase_count_pivot['index3.0 star rating'] + phrase_count_pivot['index4.0 star rating'] + phrase_count_pivot['index5.0 star rating']\n","\n","phrase_count_pivot.sort_values(by=('index1.0 star rating'), ascending=False)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_segments_unpacked</th>\n","      <th>index1.0 star rating</th>\n","      <th>index2.0 star rating</th>\n","      <th>index3.0 star rating</th>\n","      <th>index4.0 star rating</th>\n","      <th>index5.0 star rating</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>810</th>\n","      <td>service</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>30</td>\n","      <td>41</td>\n","      <td>108</td>\n","    </tr>\n","    <tr>\n","      <th>1009</th>\n","      <td>wait</td>\n","      <td>8</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>4</td>\n","      <td>17</td>\n","    </tr>\n","    <tr>\n","      <th>628</th>\n","      <td>order</td>\n","      <td>6</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>22</td>\n","    </tr>\n","    <tr>\n","      <th>534</th>\n","      <td>lukewarm</td>\n","      <td>4</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>4</td>\n","    </tr>\n","    <tr>\n","      <th>607</th>\n","      <td>not</td>\n","      <td>4</td>\n","      <td>21</td>\n","      <td>23</td>\n","      <td>16</td>\n","      <td>13</td>\n","      <td>77</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>379</th>\n","      <td>fresh</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>6</td>\n","    </tr>\n","    <tr>\n","      <th>380</th>\n","      <td>friend</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>5</td>\n","    </tr>\n","    <tr>\n","      <th>381</th>\n","      <td>friendly</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>4</td>\n","      <td>11</td>\n","    </tr>\n","    <tr>\n","      <th>382</th>\n","      <td>friends</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>2</td>\n","      <td>7</td>\n","    </tr>\n","    <tr>\n","      <th>1068</th>\n","      <td></td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1069 rows × 7 columns</p>\n","</div>"],"text/plain":["     word_segments_unpacked  index1.0 star rating  index2.0 star rating  \\\n","810                 service                    13                    11   \n","1009                   wait                     8                     1   \n","628                   order                     6                     3   \n","534                lukewarm                     4                     0   \n","607                     not                     4                    21   \n","...                     ...                   ...                   ...   \n","379                   fresh                     0                     0   \n","380                  friend                     0                     0   \n","381                friendly                     0                     0   \n","382                 friends                     0                     0   \n","1068                                            0                     1   \n","\n","      index3.0 star rating  index4.0 star rating  index5.0 star rating  score  \n","810                     13                    30                    41    108  \n","1009                     1                     3                     4     17  \n","628                      1                     5                     7     22  \n","534                      0                     0                     0      4  \n","607                     23                    16                    13     77  \n","...                    ...                   ...                   ...    ...  \n","379                      3                     1                     2      6  \n","380                      0                     1                     4      5  \n","381                      1                     6                     4     11  \n","382                      1                     4                     2      7  \n","1068                     0                     0                     0      1  \n","\n","[1069 rows x 7 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"z34HrfWSxovE","colab_type":"code","outputId":"df4419bc-1a8f-4ea0-e3ea-014fc36ff786","executionInfo":{"status":"ok","timestamp":1577991964778,"user_tz":300,"elapsed":505,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":110}},"source":["# phrase_count_pivot['sum'] = (phrase_count_pivot['sum']-phrase_count_pivot['sum'].min()/phrase_count_pivot['sum'].max()-phrase_count_pivot['sum'].min())/phrase_count_pivot['sum'].max()\n","phrase_count_pivot['score'] = phrase_count_pivot['score'].div(phrase_count_pivot['score'].max(), axis=0)\n","phrase_count_pivot['score'] = phrase_count_pivot['score'].round(decimals=4)\n","phrase_count_pivot = phrase_count_pivot.sort_values(by=('score'), ascending=False)\n","phrase_count_pivot.head(2)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>word_segments_unpacked</th>\n","      <th>index1.0 star rating</th>\n","      <th>index2.0 star rating</th>\n","      <th>index3.0 star rating</th>\n","      <th>index4.0 star rating</th>\n","      <th>index5.0 star rating</th>\n","      <th>score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>407</th>\n","      <td>great</td>\n","      <td>4</td>\n","      <td>6</td>\n","      <td>5</td>\n","      <td>30</td>\n","      <td>82</td>\n","      <td>1.0000</td>\n","    </tr>\n","    <tr>\n","      <th>810</th>\n","      <td>service</td>\n","      <td>13</td>\n","      <td>11</td>\n","      <td>13</td>\n","      <td>30</td>\n","      <td>41</td>\n","      <td>0.8504</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    word_segments_unpacked  index1.0 star rating  index2.0 star rating  \\\n","407                  great                     4                     6   \n","810                service                    13                    11   \n","\n","     index3.0 star rating  index4.0 star rating  index5.0 star rating   score  \n","407                     5                    30                    82  1.0000  \n","810                    13                    30                    41  0.8504  "]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"5cgmShkoprtn","colab_type":"code","outputId":"76afe490-feef-4900-baf4-6293bf4db1e4","executionInfo":{"status":"ok","timestamp":1577991966275,"user_tz":300,"elapsed":553,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["phrase_count_pivot['word_segments_unpacked'].iloc[-2]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'lights'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"oeVWYPKKdWaa","colab_type":"code","outputId":"f06b7068-1d13-4205-e4d9-21565e6cec5d","executionInfo":{"status":"ok","timestamp":1577991967338,"user_tz":300,"elapsed":547,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":332}},"source":["#find where the worst terms appear in the review word segments and extract that word phrase to give context\n","worst_term = df[df['word_segments_unpacked'].str.contains(phrase_count_pivot['word_segments_unpacked'].iloc[-2])]\n","worst_term = worst_term.sort_values(by=2, ascending=True)\n","worst_term.head(1)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>tokenized_text</th>\n","      <th>parts_of_speech_reference</th>\n","      <th>word_segments</th>\n","      <th>word_segments_unpacked</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>17</th>\n","      <td>\\n        11/2/2019\\n</td>\n","      <td>Hells Kitchen is having a resurgence in good restaurants and Bea is for sure among them. A large restaurant/bar, my friend and I were seated in the \"indoor-outdoor\" area featuring trees with fairy lights and Edward Scissorhands playing silently on the wall behind us. The drink menu is well rounded and features a variety of creative cocktails, the food is high quality and we found our server to be friendly and helpful. It's the kind of oasis to escape to after a long day of working in the Times Square area- IF you can get a table.</td>\n","      <td>5.0 star rating</td>\n","      <td>[Hells, Kitchen, is, having, a, resurgence, in, good, restaurants, and, Bea, is, for, sure, among, them, A, large, restaurant, bar, my, friend, and, I, were, seated, in, the, indoor, outdoor, area, featuring, trees, with, fairy, lights, and, Edward, Scissorhands, playing, silently, on, the, wall, behind, us, The, drink, menu, is, well, rounded, and, features, a, variety, of, creative, cocktails, the, food, is, high, quality, and, we, found, our, server, to, be, friendly, and, helpful, It's, the, kind, of, oasis, to, escape, to, after, a, long, day, of, working, in, the, Times, Square, area, IF, you, can, get, a, table]</td>\n","      <td>Hells Kitchen is having a resurgence in good restaurants and Bea is for sure among them A large restaurant bar my friend and I were seated in the indoor outdoor area featuring trees with fairy lights and Edward Scissorhands playing silently on the wall behind us The drink menu is well rounded and features a variety of creative cocktails the food is high quality and we found our server to be friendly and helpful It's the kind of oasis to escape to after a long day of working in the Times Square area IF you can get a table</td>\n","      <td>[[good, restaurants, and, Bea, is, for, sure], [large, restaurant, /, bar, ,, my, friend], [featuring, trees, with, fairy, lights, and, Edward], [creative, cocktails, ,, the, food, is, high], [food, is, high, quality, and, we, found], [high, quality, and, we, found, our, server], [long, day, of, working, in, the, Times]]</td>\n","      <td>[good, restaurants, and, Bea, is, for, sure, large, restaurant, /, bar, ,, my, friend, featuring, trees, with, fairy, lights, and, Edward, creative, cocktails, ,, the, food, is, high, food, is, high, quality, and, we, found, high, quality, and, we, found, our, server, long, day, of, working, in, the, Times]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            0  \\\n","17  \\n        11/2/2019\\n       \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          1  \\\n","17  Hells Kitchen is having a resurgence in good restaurants and Bea is for sure among them. A large restaurant/bar, my friend and I were seated in the \"indoor-outdoor\" area featuring trees with fairy lights and Edward Scissorhands playing silently on the wall behind us. The drink menu is well rounded and features a variety of creative cocktails, the food is high quality and we found our server to be friendly and helpful. It's the kind of oasis to escape to after a long day of working in the Times Square area- IF you can get a table.   \n","\n","                  2  \\\n","17  5.0 star rating   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        tokenized_text  \\\n","17  [Hells, Kitchen, is, having, a, resurgence, in, good, restaurants, and, Bea, is, for, sure, among, them, A, large, restaurant, bar, my, friend, and, I, were, seated, in, the, indoor, outdoor, area, featuring, trees, with, fairy, lights, and, Edward, Scissorhands, playing, silently, on, the, wall, behind, us, The, drink, menu, is, well, rounded, and, features, a, variety, of, creative, cocktails, the, food, is, high, quality, and, we, found, our, server, to, be, friendly, and, helpful, It's, the, kind, of, oasis, to, escape, to, after, a, long, day, of, working, in, the, Times, Square, area, IF, you, can, get, a, table]   \n","\n","                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         parts_of_speech_reference  \\\n","17  Hells Kitchen is having a resurgence in good restaurants and Bea is for sure among them A large restaurant bar my friend and I were seated in the indoor outdoor area featuring trees with fairy lights and Edward Scissorhands playing silently on the wall behind us The drink menu is well rounded and features a variety of creative cocktails the food is high quality and we found our server to be friendly and helpful It's the kind of oasis to escape to after a long day of working in the Times Square area IF you can get a table   \n","\n","                                                                                                                                                                                                                                                                                                                         word_segments  \\\n","17  [[good, restaurants, and, Bea, is, for, sure], [large, restaurant, /, bar, ,, my, friend], [featuring, trees, with, fairy, lights, and, Edward], [creative, cocktails, ,, the, food, is, high], [food, is, high, quality, and, we, found], [high, quality, and, we, found, our, server], [long, day, of, working, in, the, Times]]   \n","\n","                                                                                                                                                                                                                                                                                                  word_segments_unpacked  \n","17  [good, restaurants, and, Bea, is, for, sure, large, restaurant, /, bar, ,, my, friend, featuring, trees, with, fairy, lights, and, Edward, creative, cocktails, ,, the, food, is, high, food, is, high, quality, and, we, found, high, quality, and, we, found, our, server, long, day, of, working, in, the, Times]  "]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"iFu1FDaK3IUb","colab_type":"code","outputId":"16df4112-eeab-4d4f-c2ec-670a36152bb0","executionInfo":{"status":"ok","timestamp":1577991968821,"user_tz":300,"elapsed":621,"user":{"displayName":"Lily Su","photoUrl":"https://lh3.googleusercontent.com/-dxYCk9QgeIY/AAAAAAAAAAI/AAAAAAAAAxw/IaW-Bj-2K7k/s64/photo.jpg","userId":"18245831014653329821"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word_list_top_bottom = ''\n","for i in list(str(worst_term['word_segments_unpacked'])):\n","  word_list_top_bottom += i\n","for word in word_list_top_bottom:\n","  word_list = word_list_top_bottom.split(', ')\n","  for i in word_list:\n","    if phrase_count_pivot['word_segments_unpacked'].iloc[-2] == i:\n","      index = word_list.index(i)\n","      rejoined_word_list = ' '.join(word_list[max(0,index-2):min(index+5, len(word_list))])\n","print(rejoined_word_list)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["with fairy lights and Edward creative cocktails\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"fZ53RzNBH2e9","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}